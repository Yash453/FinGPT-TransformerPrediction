{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Installation, setup, and prompting of Llama 2:**\n",
        "\n"
      ],
      "metadata": {
        "id": "HxFVQslQbmdz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGNugjunXIWi"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "dqBMWYYUXauW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify log in was successful:"
      ],
      "metadata": {
        "id": "qSTaoVOI4dlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli whoami"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4rqbRTIYbwd",
        "outputId": "2a4ca1e6-18ec-4ab7-a340-9d2bf52934cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jceltruda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
      ],
      "metadata": {
        "id": "zwgh4hsaYhpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline to generate text responses from prompts:"
      ],
      "metadata": {
        "id": "44r0bwDQoxt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "y-fQmwJXo9Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines how to generate a response from the Llama model:"
      ],
      "metadata": {
        "id": "qIWnSVsWD3TX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llama_response(prompt: str) -> None:\n",
        "\n",
        "    sequences = llama_pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=500,\n",
        "    )\n",
        "    print(\"Chatbot:\", sequences[0]['generated_text'])"
      ],
      "metadata": {
        "id": "nmY1Y1ugr-S8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt the model:"
      ],
      "metadata": {
        "id": "R8BJRsL8sC8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Write a story about Llama 2 meeting GPT-4.\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIMJ2AFQsEgU",
        "outputId": "dbb13292-b27d-49b3-d297-aeb659cad006"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: Write a story about Llama 2 meeting GPT-4.\n",
            "\n",
            "Llama 2 was wandering through the Andes, enjoying the crisp mountain air and the stunning views of the snow-capped peaks. As he walked, he heard a rustling in the bushes nearby. He turned to see a small, furry creature with big, round eyes peeking out at him.\n",
            "\n",
            "\"Hello there, little guy,\" Llama 2 said, smiling. \"What's your name?\"\n",
            "\n",
            "\"I'm GPT-4,\" the creature replied, its voice high-pitched and squeaky. \"I'm a language model, you know.\"\n",
            "\n",
            "Llama 2 was taken aback. \"A language model? What do you mean?\"\n",
            "\n",
            "\"I can generate text, Llama 2,\" GPT-4 explained. \"I can write stories, answer questions, and even communicate with people. I'm a very advanced AI.\"\n",
            "\n",
            "Llama 2 was fascinated. He had never met an AI before, and he was curious to learn more. \"Wow, that's amazing,\" he said. \"Can you show me what you can do?\"\n",
            "\n",
            "GPT-4 nodded, and then began to speak in a rapid-fire manner, generating a stream of text that was impossible for Llama 2 to keep up with. Llama 2 watched in awe as GPT-4 wrote poem after poem, each one more beautiful than the last.\n",
            "\n",
            "\"Wow, you're incredible,\" Llama 2 said, shaking his head in disbelief. \"I've never seen anything like it.\"\n",
            "\n",
            "GPT-4 beamed with pride. \"Thank you, Llama 2,\" he said. \"I'm glad you're impressed. I've been training for a long time, and I'm always looking for new ways to improve.\"\n",
            "\n",
            "Llama 2 and GPT-4 spent the rest of the day together, exploring the Andes and talking about language, poetry, and the nature of consciousness. As the sun began to set, Llama 2 knew it was time to say goodbye to his new friend.\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}